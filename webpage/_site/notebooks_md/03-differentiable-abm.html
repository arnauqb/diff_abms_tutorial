<!DOCTYPE html>
<html lang="en"><head>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title> </title><!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta name="author" content="Arnau Quera-Bofarull" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A (nearly) no-css minimalist Jekyll theme." />
<meta property="og:description" content="A (nearly) no-css minimalist Jekyll theme." />
<link rel="canonical" href="http://github.com/riggraz/no-style-please/https://arnauqb.github.io/diff_abms_tutorial/notebooks_md/03-differentiable-abm.html" />
<meta property="og:url" content="http://github.com/riggraz/no-style-please/https://arnauqb.github.io/diff_abms_tutorial/notebooks_md/03-differentiable-abm.html" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Arnau Quera-Bofarull"},"description":"A (nearly) no-css minimalist Jekyll theme.","url":"http://github.com/riggraz/no-style-please/https://arnauqb.github.io/diff_abms_tutorial/notebooks_md/03-differentiable-abm.html"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="http://github.com/riggraz/no-style-please/https://arnauqb.github.io/diff_abms_tutorial/feed.xml" title=" " /><link rel="shortcut icon" type="image/x-icon" href="/https://arnauqb.github.io/diff_abms_tutorial/logo.png" />
  <link rel="stylesheet" href="assets/css/main.css" />
</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <header>
  <h1> </h1></header><ul></ul><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<h1 id="differentiable-agent-based-models">Differentiable Agent-Based Models</h1>

<p>We are now in a good spot to code our first differentiable ABM. Refer to notebooks 1 and 2 for a review of automatic differentiation (AD).</p>

<h1 id="1-random-walk">1. Random Walk</h1>

<p>Let us first consider a very simple ABM: a 1-dimensional random walk. The model is defined through the recursion</p>

\[\begin{align}
\xi &amp;\sim \mathrm{Bernoulli}({\theta}),\\
x_{t+1} &amp;= x_t + \begin{cases}1 &amp;\mathrm{if} &amp;\xi = 1 \\ -1 &amp;\mathrm{if} &amp;\xi = 0\end{cases}
\end{align}\]

<p>A first naive implementation using PyTorch might be:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">xi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">Bernoulli</span><span class="p">(</span><span class="n">theta</span><span class="p">).</span><span class="nf">sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">xi</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">next_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">next_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">next_x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x138d87f70&gt;]
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_3_1.png" alt="png" /></p>

<p>So the model seems to be working fine. Now what we are interested is in computing the jacobian</p>

\[(J)_i = \frac{\partial {x_i}}{\partial \theta}\]

<p>which we will later use for calibration. If we try to use torch’s autograd to compute the jacobian:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dx_dtheta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="o">=</span><span class="n">n_timesteps</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">dx_dtheta</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x138e7ba60&gt;]
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_5_1.png" alt="png" /></p>

<p>We obtain a gradient of 0. Why is that? There are two main reasons:</p>

<ol>
  <li>As we noted in the previous tutorials, the Bernoulli distribution is not automatically differentiable, and we need to use a continuous relaxation such as Gumbel-Softmax.</li>
  <li>AD frameworks such as PyTorch require a static computation graph to perform AD. That means that, even though they support control flow statement such as if or else, they do not support control flow statements that depend on the parameters that we want to differentiate to. This can be circumvented by using masks. That is, a statement</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">Bernoulli</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="k">if</span> <span class="n">x</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">a</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></div>

<p>can be written as</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">Bernoulli</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">xi</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">xi</span><span class="p">)</span> <span class="o">*</span> <span class="n">b</span>
</code></pre></div></div>

<p>with this in mind, we can rewrite our example as:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">theta</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)).</span><span class="nf">log</span><span class="p">()</span>
        <span class="n">xi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">next_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">xi</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">next_x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x138ff3ee0&gt;]
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_8_1.png" alt="png" /></p>

<p>Now, remember, because of the <code class="language-plaintext highlighter-rouge">hard=True</code> the forward simulation is identical to the previous case. That is, the continuous relaxation that we model with Gumbel-Softmax only affects the backward gradient propagation. Let us now recompute the jacobian:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dx_dtheta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="o">=</span><span class="n">n_timesteps</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">theta</span>
<span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">dx_dtheta</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x1494c61a0&gt;]
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_10_1.png" alt="png" /></p>

<p>and now we have a gradient! The temperature parameter of the GS distribution entails a bias-variance tradeoff as explained in the previous notebook. Let’s analyze the effect here.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">taus</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">n_gradient_samples</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">gradients_per_tau</span> <span class="o">=</span> <span class="p">{</span><span class="n">tau</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">tau</span> <span class="ow">in</span> <span class="n">taus</span><span class="p">}</span>
<span class="k">for</span> <span class="n">tau</span> <span class="ow">in</span> <span class="n">taus</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_gradient_samples</span><span class="p">):</span>
        <span class="n">dx_dtheta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="o">=</span><span class="n">n_timesteps</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">vectorize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">gradients_per_tau</span><span class="p">[</span><span class="n">tau</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">dx_dtheta</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tau</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">gradients_per_tau</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">gradients_per_tau</span><span class="p">[</span><span class="n">tau</span><span class="p">]:</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span>
        <span class="nf">sum</span><span class="p">(</span><span class="n">gradients_per_tau</span><span class="p">[</span><span class="n">tau</span><span class="p">])</span> <span class="o">/</span> <span class="n">n_gradient_samples</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span>
        <span class="c1">#linestyle="",
</span>        <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="sh">"</span><span class="s">$\tau$ = </span><span class="si">{</span><span class="n">tau</span><span class="si">}</span><span class="s"> [mean]</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="sh">"</span><span class="s">$\tau$ = </span><span class="si">{</span><span class="n">tau</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span>
    <span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span><span class="p">),</span>
    <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span><span class="p">)),</span>
    <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true gradient</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x168585660&gt;
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_13_1.png" alt="png" /></p>

<p>First let’s address how we obtained the true gradient value by the black dashed line $y=x$. Since the random walk is a linear model, we can write</p>

\[\begin{align}
\frac{\partial}{\partial \theta} \mathbb E[x_N]  &amp; = \frac{\partial}{\partial \theta} \mathbb E\left[ \sum_{j=1}^{N} 2 \mathrm{Bernoulli(\theta)} - 1 \right]\\
                                                 &amp; = \frac{\partial}{\partial \theta} (2  N  \theta - N) \\
                                                 &amp; = 2 N
\end{align}\]

<p>Second, we observe that for decreasing values of $\tau$, the mean of the gradient estimate gets closer to the true values, but the variance increases significantly. In this particular case, we may do better with $\tau=0.5$ than $\tau=0.1$ since the small bias is an acceptable trade for a big reduction in variance.</p>

<h1 id="2-sir-model">2. SIR model</h1>

<p>Let us know code a differentiable Susceptible-Infected-Recovered epidemiological model. The ABM is a discretization of the system of equations</p>

\[\begin{align}
\frac{\mathrm{d} S}{\mathrm{d} t} &amp;= - \beta SI \\
\frac{\mathrm{d} I}{\mathrm{d} t} &amp;=  \beta SI - \gamma I\\
\frac{\mathrm{d} R}{\mathrm{d} t} &amp;= \gamma I \\
\end{align}\]

<p>where $\beta$ is the effective contact rate (higher values correspond to faster disease spread), and $\gamma$ is the recovery rate (e.g., a $\gamma =0.05$ corresponds to a mean recovery time of 20 days), and S, I, R are the fraction of susceptible, infected, and recovered individuals respectively.</p>

<p>The corresponding agent-based model can be obtained by considering a collection of $N$ agents. At each time-step, the probability of agent $i$ getting infected is given by</p>

\[p_i = 1 - \exp{\left(-\beta I \Delta t\right)},\]

<p>where $I$ is the fraction of individuals infected at this time, and $\Delta t$ is the duration of the time-step. Likewise, an infected individual can recover with probability</p>

\[q_i = 1 - \exp{\left(-\gamma \Delta t\right)}.\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_bernoulli</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">]).</span><span class="nf">t</span><span class="p">().</span><span class="nf">log</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">sir</span><span class="p">(</span>
    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">initial_fraction_infected</span><span class="p">,</span> <span class="n">n_agents</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">delta_t</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">):</span>
    <span class="c1"># here S, I, R denote arrays of size (n_agents, ) with 1 or 0 depending on their state.
</span>    <span class="n">I</span> <span class="o">=</span> <span class="nf">sample_bernoulli</span><span class="p">(</span><span class="n">initial_fraction_infected</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n_agents</span><span class="p">),</span> <span class="n">tau</span><span class="p">)</span>
    <span class="n">S</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">I</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">n_agents</span><span class="p">)</span>
    <span class="n">infections_per_timestep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([])</span>
    <span class="n">recoveries_per_timestep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span><span class="p">):</span>
        <span class="c1"># sample probs
</span>        <span class="n">probs_infected</span> <span class="o">=</span> <span class="n">S</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span> <span class="o">*</span> <span class="nf">sum</span><span class="p">(</span><span class="n">I</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_agents</span> <span class="o">*</span> <span class="n">delta_t</span><span class="p">))</span>
        <span class="n">probs_infected</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">probs_infected</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">is_infected</span> <span class="o">=</span> <span class="nf">sample_bernoulli</span><span class="p">(</span><span class="n">probs_infected</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">probs_recovery</span> <span class="o">=</span> <span class="n">I</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">delta_t</span><span class="p">))</span>
        <span class="n">probs_recovery</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">probs_recovery</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">is_recovered</span> <span class="o">=</span> <span class="nf">sample_bernoulli</span><span class="p">(</span><span class="n">probs_recovery</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
        <span class="c1"># update
</span>        <span class="n">S</span> <span class="o">=</span> <span class="n">S</span> <span class="o">-</span> <span class="n">is_infected</span>
        <span class="n">I</span> <span class="o">=</span> <span class="n">I</span> <span class="o">+</span> <span class="n">is_infected</span> <span class="o">-</span> <span class="n">is_recovered</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">is_recovered</span>
        <span class="c1"># save
</span>        <span class="n">infections_per_timestep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span>
            <span class="p">(</span><span class="n">infections_per_timestep</span><span class="p">,</span> <span class="n">is_infected</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">n_agents</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">recoveries_per_timestep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span>
            <span class="p">(</span><span class="n">recoveries_per_timestep</span><span class="p">,</span> <span class="n">is_recovered</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">n_agents</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">infections_per_timestep</span><span class="p">,</span> <span class="n">recoveries_per_timestep</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">initial_fraction_infected</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">n_agents</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">delta_t</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">inf_t</span><span class="p">,</span> <span class="n">rec_t</span> <span class="o">=</span> <span class="nf">sir</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">initial_fraction_infected</span><span class="p">,</span> <span class="n">n_agents</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">delta_t</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">inf_t</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Daily infections</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rec_t</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Daily recoveries</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time [ day ]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Population fraction</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x169415de0&gt;
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_18_1.png" alt="png" /></p>

<h2 id="22-gradients-of-the-sir-model">2.2 Gradients of the SIR model</h2>

<p>Similarly as the random walk model, we can now easily obtain the gradients of the infections and recoveries time-series.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># fixed parameters
</span><span class="n">initial_fraction_infected</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">n_agents</span> <span class="o">=</span> <span class="mi">1_000</span>
<span class="n">delta_t</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">60</span>
<span class="k">def</span> <span class="nf">faux</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="nf">sir</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">initial_fraction_infected</span><span class="p">,</span> <span class="n">n_agents</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">delta_t</span><span class="p">)</span>

<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">jacobian</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span><span class="n">faux</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">]),</span> <span class="n">vectorize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">jacobian</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily infections</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">jacobian</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily recoveries</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">Gradients resp. to $\beta$</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">jacobian</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily infections</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">jacobian</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily recoveries</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">Gradients resp. to $\gamma $</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time [ day ]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Gradient</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'Gradient')
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_21_1.png" alt="png" /></p>

<h2 id="23-fitting-by-gradient-descent">2.3 Fitting by gradient descent</h2>

<p>It’s about time to do what we set up to do with this tutorial: using the gradients to calibrate our model.</p>

<p>Let’s assume we have an observed (multivariate) time-series $\mathbf y$. We want to compute the “optimal” values of $\beta$ and $\gamma$ that generate $\mathbf x$ that is as close as possible as $\mathbf y$. More specifically, we want to compute</p>

\[(\beta^*, \gamma^*) = \argmin_{(\beta, \gamma)} \ell \, \left(\mathbf x(\beta, \gamma), \mathbf y\right)\]

<p>where $\ell$ is an appropritate distance function. In this case, we will just consider the standard L2 loss.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># let's generate some fake observation data first
</span><span class="n">true_beta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">true_gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">true_inf</span><span class="p">,</span> <span class="n">true_rec</span> <span class="o">=</span> <span class="nf">sir</span><span class="p">(</span>
    <span class="n">true_beta</span><span class="p">,</span> <span class="n">true_gamma</span><span class="p">,</span> <span class="n">initial_fraction_infected</span><span class="p">,</span> <span class="n">n_agents</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">delta_t</span>
<span class="p">)</span>
<span class="c1"># let's add some observation noise
</span><span class="n">sigma_obs</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">true_inf</span> <span class="o">=</span> <span class="n">true_inf</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_timesteps</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma_obs</span> <span class="o">*</span> <span class="n">true_inf</span>
<span class="n">true_rec</span> <span class="o">=</span> <span class="n">true_rec</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_timesteps</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma_obs</span> <span class="o">*</span> <span class="n">true_rec</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">true_inf</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily infections</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">true_rec</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily recoveries</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x169d4b4f0&gt;]
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_23_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># now let's find the best fit through gradient descent and try to recover the true parameters
# one thing to consider is that gradient descent is not a constrained optimization method
# so to avoid beta and gamma to go negative we will use a log transformation
</span><span class="n">log_beta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">log_gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">log_beta</span><span class="p">,</span> <span class="n">log_gamma</span><span class="p">,</span> <span class="n">obs_inf</span><span class="p">,</span> <span class="n">obs_rec</span><span class="p">):</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">log_beta</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">log_gamma</span>
    <span class="n">inf</span><span class="p">,</span> <span class="n">rec</span> <span class="o">=</span> <span class="nf">sir</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">initial_fraction_infected</span><span class="p">,</span> <span class="n">n_agents</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">delta_t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">inf</span> <span class="o">-</span> <span class="n">obs_inf</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">rec</span> <span class="o">-</span> <span class="n">obs_rec</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">loss_hist</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">beta_hist</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">gamma_hist</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">optimizer</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">log_beta</span><span class="p">,</span> <span class="n">log_gamma</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">l</span> <span class="o">=</span> <span class="nf">loss</span><span class="p">(</span><span class="n">log_beta</span><span class="p">,</span> <span class="n">log_gamma</span><span class="p">,</span> <span class="n">true_inf</span><span class="p">,</span> <span class="n">true_rec</span><span class="p">)</span>
    <span class="n">l</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="c1"># clip norm this is important to avoid exploding gradients
</span>    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">([</span><span class="n">log_beta</span><span class="p">,</span> <span class="n">log_gamma</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="n">loss_hist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
    <span class="n">beta_hist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="n">log_beta</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
    <span class="n">gamma_hist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="n">log_gamma</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 200/200 [01:27&lt;00:00,  2.28it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># let's see the results
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">loss_hist</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Loss</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Loss</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Estimated parameters</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">beta_hist</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Estimated beta</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">"</span><span class="s">C0</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">gamma_hist</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Estimated gamma</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">"</span><span class="s">C1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axhline</span><span class="p">(</span><span class="n">true_beta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">C0</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">True beta</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axhline</span><span class="p">(</span><span class="n">true_gamma</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">C1</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">True gamma</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x31a3ee2c0&gt;
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_25_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

      </div>
    </main>
  </body>
</html>