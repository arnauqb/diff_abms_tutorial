<!DOCTYPE html>
<html lang="en"><head>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title> </title><!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.3" />
<meta name="author" content="Arnau Quera-Bofarull" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A (nearly) no-css minimalist Jekyll theme." />
<meta property="og:description" content="A (nearly) no-css minimalist Jekyll theme." />
<link rel="canonical" href="http://github.com/riggraz/no-style-please/https://arnauqb.github.io/diff_abms_tutorial/04-variational-inference.html" />
<meta property="og:url" content="http://github.com/riggraz/no-style-please/https://arnauqb.github.io/diff_abms_tutorial/04-variational-inference.html" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Arnau Quera-Bofarull"},"description":"A (nearly) no-css minimalist Jekyll theme.","url":"http://github.com/riggraz/no-style-please/https://arnauqb.github.io/diff_abms_tutorial/04-variational-inference.html"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="http://github.com/riggraz/no-style-please/https://arnauqb.github.io/diff_abms_tutorial/feed.xml" title=" " /><link rel="shortcut icon" type="image/x-icon" href="/https://arnauqb.github.io/diff_abms_tutorial/logo.png" />
  <link rel="stylesheet" href="assets/css/main.css" />
</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <header>
  <h1> </h1></header><ul></ul><h1 id="bayesian-calibration-of-agent-based-models-with-variational-inference">Bayesian calibration of Agent-Based Models with Variational Inference</h1>

<p>In this tutorial we will show how to use the <code class="language-plaintext highlighter-rouge">blackbirds</code> package to calibrate an Agent-Based Model (ABM) using Variational Inference (VI). 
Let’s start by providing a short introduction to VI. Given an ABM with parameters $\theta$ and observed data $y$, the goal of VI is to approximate the posterior distribution $p(\theta|y)$ with a simpler distribution parameterized by $\phi$, $q_\phi(\theta)$, that is easier to work with. The optimal $q^<em>(\theta)$ is the one with parameters $\phi^</em>$ that minimize the Kullback-Leibler divergence between $q_\phi(\theta)$ and $p(\theta|y)$.</p>

<p>It can be shown <a href="https://arxiv.org/pdf/1904.02063.pdf">[see ref]</a> that given $n$ observations $x_{1:n}$, a prior distribution over the parameters $\theta$, a choice of variational family $q\in \mathcal Q$, and a loss function $\ell (\theta, x_i)$, the optimal $q_*(\theta)$ is given by</p>

\[q_*(\theta) = \argmin_{q\in \mathcal Q} \left\{ \mathbb E_{q(\theta)} \left[ \sum_{i=1}^n \ell (\theta, x_i) \right] + D(q||\pi) \right\}.\]

<table>
  <tbody>
    <tr>
      <td>In standard VI, the loss function is the negative log-likelihood of the data, $\ell (\theta, x_i) = -\log p(x_i</td>
      <td>\theta)$, and distance measure $D$ is the Kullback-Leibler divergence. In the case of ABMs, the likelihood function is often intractable, and we need to resort to alternative loss functions such as the $L_2$ distance between the observed data and the ABM output. This variant of VI is known as Generalized Variational Inference (GVI) <a href="https://arxiv.org/pdf/1904.02063.pdf">[see ref]</a>.</td>
    </tr>
  </tbody>
</table>

<p>Luckily, we do not need to worry too much about the details of the optimization process, as the <code class="language-plaintext highlighter-rouge">blackbirds</code> package takes care of that for us. We only need to provide the ABM, the observed data, and the prior distribution over the parameters. Let’s see how this works in practice.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#!pip install blackbirds
</span></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">blackbirds.infer.vi</span> <span class="kn">import</span> <span class="n">VI</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[HAL-9093.local:14032] shmem: mmap: an error occurred while determining whether or not /var/folders/s5/jmvhvqns52q3ysypfjykg6y40000gr/T//ompi.HAL-9093.504/jf.0/2144075776/sm_segment.HAL-9093.504.7fcc0000.0 could be created.
</code></pre></div></div>

<h1 id="2-example-calibration-of-a-random-walk">2. Example: Calibration of a Random Walk</h1>

<p>In the previous tutorial we implemented a differentiable Random Walk model. We now make a small modification by transforming the $\theta$ parameter to keep it in $[0, 1]$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="c1"># to keep it phyiscal
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">theta</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)).</span><span class="nf">log</span><span class="p">()</span>
        <span class="n">xi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">next_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">xi</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">next_x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>Let’s generate some synthetic data and use the <code class="language-plaintext highlighter-rouge">blackbirds</code> package to calibrate the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Generate synthetic data
</span><span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">theta_true</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">logit</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.2</span><span class="p">]))</span>
<span class="n">x_true</span> <span class="o">=</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta_true</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">True</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x17bd744f0&gt;]
</code></pre></div></div>

<p><img src="04-variational-inference_files/04-variational-inference_6_1.png" alt="png" /></p>

<p>We will use a Gaussian prior over $\theta$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prior</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">logit</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)),</span> <span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we need to choose a variational family for $q$. We will use a Normal distribution with learnable mean and variance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TrainableGaussian</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">rsample</span><span class="p">((</span><span class="n">n</span><span class="p">,))</span>
        <span class="n">logprob</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">log_prob</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">logprob</span>
</code></pre></div></div>

<p>Now we need to specify the loss $\ell(\theta, y)$. We will use the $L_2$ distance between the observed data and the model output:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>And we have all the ingredients!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">q</span> <span class="o">=</span> <span class="nc">TrainableGaussian</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">vi</span> <span class="o">=</span> <span class="nc">VI</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span>
    <span class="n">posterior_estimator</span><span class="o">=</span><span class="n">q</span><span class="p">,</span>
    <span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">w</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="c1"># this is a relative weight between the loss and the KL term
</span>    <span class="p">)</span>
<span class="n">vi</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">x_true</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_epochs_without_improvement</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 100/100 [00:18&lt;00:00,  5.55it/s, loss=40.53, reg=178.80, total=219.34, best=199.34, stall=21]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># we can now inspect the loss
</span><span class="n">loss_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">vi</span><span class="p">.</span><span class="n">losses_hist</span><span class="p">)</span>
<span class="n">loss_df</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="sh">'</span><span class="s">Loss</span><span class="sh">'</span><span class="p">);</span>
</code></pre></div></div>

<p><img src="04-variational-inference_files/04-variational-inference_15_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># and we can check the trained posterior
</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">prior_samples</span> <span class="o">=</span> <span class="n">prior</span><span class="p">.</span><span class="nf">sample</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,)).</span><span class="nf">numpy</span><span class="p">()</span>
<span class="n">posterior_samples</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">detach</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">prior_samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Prior</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">posterior_samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Posterior</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">theta_true</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">True</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">();</span>
</code></pre></div></div>

<p><img src="04-variational-inference_files/04-variational-inference_16_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

      </div>
    </main>
  </body>
</html>